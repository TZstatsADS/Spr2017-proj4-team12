---
title: "ADS Project 4 Team12"
author: "Kexin Nie, Kai Chen, Senyao Han, Yini Zhang, Chenyun Zhu"
date: "April 13, 2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Introduction
In this entity resolution project. We studied two papers (No.2 and No.5) about entity resolution and tried to understand their algorism. We also compared the two methods. We believe that our work is significant for people who want to have a better understand of these two papers. The following are the details of our works.

# Part I Simple Linear Support Vector Machine
SVM is one of the most popular tools of classification. In paper 2, the author introduced the use of simple linear svm for a multi-classification. They first divided the data set into test and training set by seperating data with same label into two groups with same size. Then they calculate the test prediction errors. The accuracy rate is simply the porpotion of right match in the test set.
For the features selection, since the author of paper doesn't point out a specific way to choose features, we decide to use the sample code on Github. Also, at the end we caculated the mean and standard deviation of accuracy of all our data set and compared the performance of variable Coauthor, Paper and Journal.
## 1.0 Packages Install
```{r}
install.packages("pacman")
pacman::p_load(text2vec, dplyr, qlcMatrix, kernlab, knitr)
install.packages("e1071")
library(e1071)
```
## 1.1 Input data and feature selection

```{r}
library(pacman)
#Our feature fuction
get_feature<-function(data, condition="combine"){
  
  if (condition=="coauthor"){
    it_train <- itoken(data$Coauthor,
                       preprocessor = tolower,
                       tokenizer = word_tokenizer,
                       ids = data$PaperID,
                       # turn off progressbar because it won't look nice in rmd
                       progressbar = FALSE)
  }else if(condition=="paper"){
    it_train <- itoken(data$Paper,
                       preprocessor = tolower,
                       tokenizer = word_tokenizer,
                       ids = data$PaperID,
                       # turn off progressbar because it won't look nice in rmd
                       progressbar = FALSE)
  } else if(condition=="journal"){
    it_train <- itoken(data$Journal,
                       preprocessor = tolower,
                       tokenizer = word_tokenizer,
                       ids = data$PaperID,
                       # turn off progressbar because it won't look nice in rmd
                       progressbar = FALSE)
  } else{
    data$combine<-paste(data$Paper, data$Coauthor, data$Journal)
    it_train <- itoken(data$combine,
                       preprocessor = tolower,
                       tokenizer = word_tokenizer,
                       ids = data$PaperID,
                       # turn off progressbar because it won't look nice in rmd
                       progressbar = FALSE)
  }
  
  vocab <- create_vocabulary(it_train, stopwords = c("a", "an", "the", "in", "on",
                                                     "at", "of", "above", "under"))
  #vocab
  vectorizer <- vocab_vectorizer(vocab)
  dtm_train <- create_dtm(it_train, vectorizer)
  tfidf <- TfIdf$new()
  dtm_train_tfidf <- fit_transform(dtm_train, tfidf)
  docsdissim <- cosSparse(t(dtm_train_tfidf))
  y<-as.data.frame(as.matrix(docsdissim))
  y$label<-as.factor(data$AuthorID)
  return(y)
  
}

my.dir<-"C:/Users/kn2403/Downloads/Spr2017-proj4-team12-master (2)/Spr2017-proj4-team12-master/output/Coauthor_No_Space_Author"
files<-list.files(my.dir)
path<-file.path(my.dir, files)
n<-length(path)
names<-gsub(".csv", "", files)

Data<-list()
n<-1
for (i in path){
  Data[[n]]<-read.csv(i, header = T, stringsAsFactors = FALSE)
  n<-n+1
}

feature_paper<-list()
feature_coauthor<-list()
feature_journal<-list()
feature_combine<-list()
feature_paper<-lapply(Data, get_feature, condition="paper")
feature_coauthor<-lapply(Data, get_feature, condition="coauthor")
feature_journal<-lapply(Data, get_feature, condition="journal")
feature_combine<-lapply(Data, get_feature)

feature_coauthor[is.na(feature_coauthor)]<-0
feature_combine[is.na(feature_combine)]<-0

entity<-function(data){
  en<-nrow(data)/length(unique(data$AuthorID))
  return(en)}

mean_entity<-lapply(Data, entity)
```
## 1.2 Linear SVM
To understand the basic idea of svm, please visit:
http://www.cs.columbia.edu/~kathy/cs4701/documents/jason_svm_tutorial.pdf

```{r}
get_svm_result<-function(data){
library(e1071)
train<-c()
test<-c()
author<-unique(data$label)
for (i in author){
  d<-data[data$label==i,]
  n<-nrow(d)
  c<-0.5*n
  sample<-sample(1:n,c)
  test<-rbind(test, d[sample,])
  train<-rbind(train, d[-sample,])
}
#Tune 
tuneResult <- tune(svm, label ~ .,  data = train, kernel="linear",
                     ranges = list(gamma = 10^(-6:-1), cost = 10^(-1:1)))
bestmodel <- tuneResult$best.model
g<-bestmodel$gamma
c<-bestmodel$cost

model1<-svm(label~.,data=train,gamma=g,cost=c, kernel="linear")
svm.pre<-predict(model1,test)
test.error<-sum(svm.pre!=test$label)/nrow(test)
Accuracy=1-test.error


return(Accuracy)

}
```

```{r}
start.time <- Sys.time()
result_paper<-lapply(feature_paper, get_svm_result)
end.time <- Sys.time()
time_svm_paper<- end.time - start.time

start.time <- Sys.time()
result_coauthor<-lapply(feature_coauthor, get_svm_result)
end.time <- Sys.time()
time_svm_coauthor<- end.time - start.time

start.time <- Sys.time()
result_journal<-lapply(feature_journal, get_svm_result)
end.time <- Sys.time()
time_svm_journal<- end.time - start.time

start.time <- Sys.time()
result_combine<-lapply(feature_combine, get_svm_result)
end.time <- Sys.time()
time_svm_combine<- end.time - start.time
```


## 1.3 Results and Evaluation
As the results shown below, we can see column XXXXX has the best performance among the three columns. As we can see in the table, the combination feature performs the best, however, it tasks the longest average time per author.
```{r}
result_p<-unlist(result_paper)
mean_paper<-mean(result_p)
sd_paper<-sd(result_p)
mean_time_p<-time_svm_paper/14

result_j<-unlist(result_journal)
mean_journal<-mean(result_j)
sd_journal<-sd(result_j)
mean_time_j<-time_svm_journal/14

result_coauthor<-unlist(result_coauthor)
mean_coauthor<-mean(result_coauthor)
sd_coauthor<-sd(result_coauthor)
mean_time_coa<-time_svm_coauthor/14

result_combine<-unlist(result_combine)
mean_combine<-mean(result_combine)
sd_combine<-sd(result_combine)
mean_time_com<-time_svm_combine/14

result.all<-cbind(result_p,result_j,result_coauthor,result_combine)
result.all<-rbind(result.all,c(mean_paper,mean_journal,mean_coauthor,mean_combine),c(sd_paper,sd_journal,sd_coauthor,sd_combine))
names<-c(names,"Average Accuracy","SD Accuracy")
rownames(result.all)<-names
colnames(result.all)<-c("Paper","Journal","Coauthor","Paper+Journal+Coauthor")
kable(result.all,caption="Comparison of different features using SVM accuracy",digits=2)
runtime<-data.frame(matrix(c(mean_time_p,mean_time_j,mean_time_coa,mean_time_com),nrow=1,ncol=4))
colnames(runtime)<-c("Paper","Journal","Coauthor","Paper+Journal+Coauthor")
kable(runtime,caption="Comparison of different features using running time(sec) per author")

```

# Part II (Please insert Title)

# Camparism and Conclusion
Since the two methods use different feature method, we decide to use same test and training set to compare their accuracy, which is simply the match rate of on the test set. Also, we only compare the best result from svm part, which is the combined features using coauthor, journal and paper, with the other method.
```{r}
```