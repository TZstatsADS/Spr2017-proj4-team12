---
title: "ADS Project 4 Team12"
author: "Kexin Nie, Kai Chen, Senyao Han, Yini Zhang, Chenyun Zhu"
date: "April 13, 2017"
output: html_document
---

# Introduction
In this entity resolution project. We studied two papers (No.2 and No.5) about entity resolution and tried to understand their algorism. We also compared the two methods. We believe that our work is significant for people who want to have a better understand of these two papers. The following are the details of our works.

# Part I Simple Linear Support Vector Machine
SVM is one of the most popular tools of classification. In paper 2, the author introduced the use of simple linear svm for a multi-classification. They first divided the data set into test and training set by seperating data with same label into two groups with same size. Then they calculate the test prediction errors. The accuracy rate is simply the porpotion of right match in the test set.
For the features selection, since the author of paper doesn't point out a specific way to choose features, we decide to use the sample code on Github. Also, at the end we caculated the mean and standard deviation of accuracy of all our data set and compared the performance of variable Coauthor, Paper and Journal.

## 1.0 Packages Install
```{r}
install.packages("pacman")
pacman::p_load(text2vec, dplyr, qlcMatrix, kernlab, knitr)
install.packages("e1071")

```

## 1.1 Input data and feature selection

```{r}
library(e1071)
library(pacman)
#Our feature fuction
get_feature<-function(data, condition="combine"){
  if (condition=="coauthor"){
    it_train <- itoken(data$Coauthor,
                       preprocessor = tolower,
                       tokenizer = word_tokenizer,
                       ids = data$PaperID,
                       # turn off progressbar because it won't look nice in rmd
                       progressbar = FALSE)
  }else if(condition=="paper"){
    it_train <- itoken(data$Paper,
                       preprocessor = tolower,
                       tokenizer = word_tokenizer,
                       ids = data$PaperID,
                       # turn off progressbar because it won't look nice in rmd
                       progressbar = FALSE)
  } else if(condition=="journal"){
    it_train <- itoken(data$Journal,
                       preprocessor = tolower,
                       tokenizer = word_tokenizer,
                       ids = data$PaperID,
                       # turn off progressbar because it won't look nice in rmd
                       progressbar = FALSE)
  } else{
    combine<-paste(data$Paper, data$Coauthor, data$Journal)
    it_train <- itoken(combine,
                       preprocessor = tolower,
                       tokenizer = word_tokenizer,
                       ids = data$PaperID,
                       # turn off progressbar because it won't look nice in rmd
                       progressbar = FALSE)
  }
  
  vocab <- create_vocabulary(it_train, stopwords = c("a", "an", "the", "in", "on",
                                                     "at", "of", "above", "under"))
  #vocab
  vectorizer <- vocab_vectorizer(vocab)
  dtm_train <- create_dtm(it_train, vectorizer)
  tfidf <- TfIdf$new()
  dtm_train_tfidf <- fit_transform(dtm_train, tfidf)
  docsdissim <- cosSparse(t(dtm_train_tfidf))
  y<-as.data.frame(as.matrix(docsdissim))
  y$label<-as.factor(data$AuthorID)
  return(y)
  #result_sclust <- specc(as.matrix(dtm_train_tfidf),
  # centers=length(unique(data$AuthorID)))
  #return(result_sclust)
  
}

my.dir<-"../output/Coauthor_No_Space_Author"
files<-list.files(my.dir)
path<-file.path(my.dir, files)
n<-length(path)
names<-gsub(".csv", "", files)

Data<-list()
n<-1
for (i in path){
  Data[[n]]<-read.csv(i, header = T, stringsAsFactors = FALSE)
  n<-n+1
}

feature_paper<-list()
feature_coauthor<-list()
feature_journal<-list()
feature_combine<-list()
feature_paper<-lapply(Data, get_feature, condition="paper")
feature_coauthor<-lapply(Data, get_feature, condition="coauthor")
feature_journal<-lapply(Data, get_feature, condition="journal")
feature_combine<-lapply(Data, get_feature)

feature_coauthor[is.na(feature_coauthor)]<-0
feature_combine[is.na(feature_combine)]<-0
```
```{r}
entity<-function(data){
  en<-nrow(data)/length(unique(data$AuthorID))
  return(en)
}
```
```{r}
mean_entity<-lapply(Data, entity)
mean_entity<-unlist(mean_entity)
```
## 1.2 Linear SVM
To understand the basic idea of svm, please visit:
http://www.cs.columbia.edu/~kathy/cs4701/documents/jason_svm_tutorial.pdf

```{r}
get_svm_result<-function(data){
library(e1071)
train<-c()
test<-c()
author<-unique(data$label)
for (i in author){
  d<-data[data$label==i,]
  n<-nrow(d)
  c<-0.5*n
  sample<-sample(1:n,c)
  test<-rbind(test, d[sample,])
  train<-rbind(train, d[-sample,])
}
#Tune 

tuneResult <- tune(svm, label ~ .,  data = train, kernel="linear",
                     ranges = list(gamma = 10^(-6:-1), cost = 10^(-1:1)))

bestmodel <- tuneResult$best.model
g<-bestmodel$gamma
c<-bestmodel$cost
#svm.pre<-predict(bestmodel,test)
model1<-svm(label~.,data=train,gamma=g,cost=c, kernel="linear")
svm.pre<-predict(model1,test)
test.error<-sum(svm.pre!=test$label)/nrow(test)
Accuracy=1-test.error
#CV Errors
#model<-svm(label~.,data=train,gamma=g,cost=c, cross=5)
#Ave.acc<-model$tot.accuracy
#SD.acc<-sd(model$accuracies)

return(Accuracy)

}
```

```{r}
start.time <- Sys.time()
result_paper<-lapply(feature_paper, get_svm_result)
end.time <- Sys.time()
time_svm_paper<- end.time - start.time

start.time <- Sys.time()
result_coauthor<-lapply(feature_coauthor, get_svm_result)
end.time <- Sys.time()
time_svm_coauthor<- end.time - start.time

start.time <- Sys.time()
result_journal<-lapply(feature_journal, get_svm_result)
end.time <- Sys.time()
time_svm_journal<- end.time - start.time

start.time <- Sys.time()
result_combine<-lapply(feature_combine, get_svm_result)
end.time <- Sys.time()
time_svm_combine<- end.time - start.time
```


## 1.3 Results and Evaluation
As the results shown below, we can see column "paper+journal+coauthor" has the best performance among the 4 columns.
```{r}
result_p<-unlist(result_paper)
mean_paper<-mean(result_p)
sd_paper<-sd(result_p)
mean_time<-time_svm_journal/14

result_j<-unlist(result_journal)
mean_journal<-mean(result_j)
sd_journal<-sd(result_j)
mean_time<-time_svm_journal/14

result_coauthor<-unlist(result_coauthor)
mean_coauthor<-mean(result_coauthor)
sd_coauthor<-sd(result_coauthor)
mean_time<-time_svm_coauthor/14

result_combine<-unlist(result_combine)
mean_combine<-mean(result_combine)
sd_combine<-sd(result_combine)
mean_time<-time_svm_combine/14

result.all<-cbind(result_p,result_j,result_coauthor,
                  result_combine, mean_entity)
mean_t<-c(as.numeric(mean_time_p)*60,as.numeric(mean_time_j)*60,
          as.numeric(mean_time_coa),as.numeric(mean_time_com)*60, NA)
result.all<-rbind(result.all,c(mean_paper,mean_journal,
                               mean_coauthor,mean_combine, NA),
                  c(sd_paper,sd_journal,sd_coauthor,sd_combine, NA),
                  mean_t)
names1<-c(names,"Average Accuracy","SD Accuracy", "Average Run Time")
rownames(result.all)<-names1
colnames(result.all)<-c("Paper","Journal","Coauthor","Paper+Journal+Coauthor", 
                        "Average # of Paper Under Entity")
kable(result.all,
      caption="Comparison of different features using SVM accuracy",
      digits=2)

#runtime<-data.frame(matrix(c(mean_time_p,mean_time_j,mean_time_coa,mean_time_com),nrow=1,ncol=4))
#colnames(runtime)<-c("Paper","Journal","Coauthor","Paper+Journal+Coauthor")
#kable(runtime,caption="Comparison of different features using running time(sec) per author")

```

```{r}
result.a<-as.data.frame(result.all)
result.a
```


# Part II Error-driven Machine Learning with a Ranking Loss Function

## 1.1 Input data and feature selection

We selected three features for baseline model: cosine similarity of coauthors, cosine similarity of papers, and cosine similarity of journals.
We selected another three features for improved model: Mean distance between coauthors, Mean distance between papers, Mean distance between journals.
Other features considered (but finally not included due to non-significance): Min/Max distance between coauthors, Min/Max distance between papers, Min/Max distance between journals.

```{r}
# The function used to merge the multiple columns descriptions for the same label_hat
mergesomerows  <- function(cluster.id, mergevector, label_hat){
  paste(mergevector[cluster.id == label_hat], collapse = " ")
}

# Input: raw_data and label_hat
# Output: merged_matrix and unique_label_hat
# The function merged Coauthor, Paper and Journal for the same label_hat
cluster_merge <- function(raw_data, label_hat){
  unique_label_hat <- unique(label_hat)
  
  Coauthor <- sapply(unique_label_hat, 
               mergesomerows, 
               mergevector = raw_data$Coauthor, label_hat = label_hat)
  
  Paper <- sapply(unique_label_hat, 
                     mergesomerows, 
                     mergevector = raw_data$Paper, label_hat = label_hat)
  
  Journal <- sapply(unique_label_hat, 
                     mergesomerows, 
                     mergevector = raw_data$Journal, label_hat = label_hat)
  merged_matrix <- cbind(Coauthor, Paper, Journal)
  return(list(Clustered = merged_matrix, matrix_label = unique_label_hat))
}

# We calculated the cosine_similarity between different clusters
# and ouput the list of cluster id and cosine similarity matrix
cosine_similarity <- function(data){
 
  feature <- list()
  for (i in 1:3){
    it_train <- itoken(data$Clustered[,i],
                       preprocessor = tolower,
                       tokenizer = word_tokenizer,
                       # ids = data$PaperID,
                       # turn off progressbar because it won't look nice in rmd
                       progressbar = FALSE)
    vocab <- create_vocabulary(it_train, stopwords = c("a", "an", "the", "in", "on",
                                                       "at", "of", "above", "under"))
    #vocab
    vectorizer <- vocab_vectorizer(vocab)
    dtm_train <- create_dtm(it_train, vectorizer)
    tfidf <- TfIdf$new()
    dtm_train_tfidf <- fit_transform(dtm_train, tfidf)
    ftfidf<-as.matrix(dtm_train_tfidf)
    feature[[i]] <- as.matrix(cosSparse(t(ftfidf)))
  }
  # feature$label<-factor(data$AuthorID)
  return(list(CLUSTER.ID = data$matrix_label, MATRIX = feature))
}


# Function that maps a text to vector of 50 dimensions, returns a matrix
# Input: text (a vetor of words), word_vector (a matrix that maps each word to 50 dimensions) 
# Output: a matrix that maps a text (combination of words) to 50 dimensions
text_vector_function <- function(text, word_vector){
  text_vec_matrix <- c()
  for (each in unique(text)){
    if (sum(rownames(word_vector)==tolower(each))!=0) {
      index <- which(rownames(word_vector)==tolower(each))
      text_vec_matrix <- rbind(text_vec_matrix,word_vector[index, ])
    }
  }
  return(text_vec_matrix)
}

# Function that calculate the Min/max/mean vectorized text
# Input: data (a list)
# Output: a list
text_matrix_function <- function(data){
  text_output <- list()
  raw_data <- data$Clustered
  
  for (i in 1:3){
    it_train <- itoken(raw_data[,i],
                       preprocessor = tolower,
                       tokenizer = space_tokenizer,
                       progressbar = FALSE)
    vocab <- create_vocabulary(it_train, stopwords = c("a", "an", "the", "in", "on",
                                                       "at", "of", "above", "under"))
    
    vectorizer <- vocab_vectorizer(vocab, 
                                   # don't vectorize input
                                   grow_dtm = FALSE, 
                                   # use window of 5 for context words
                                   skip_grams_window = 1L)
    
    tcm <- create_tcm(it_train, vectorizer)
    glove <- GlobalVectors$new(word_vectors_size = 50, vocabulary = vocab, x_max = 10)
    suppressMessages(fit(tcm, glove, n_iter = 20))
    word_vectors <- glove$get_word_vectors()
  
    text_matrix_min <- matrix(NA, nrow = nrow(raw_data), ncol = 50)
    text_matrix_max <- matrix(NA, nrow = nrow(raw_data), ncol = 50) 
    text_matrix_mean <- matrix(NA, nrow = nrow(raw_data), ncol = 50) 
    for (j in (1:nrow(raw_data))){
      text_vector <- unlist(strsplit(raw_data[,i][j], split = " ")) 
      # row j, column i, a vector of strings
      text_vectorized <- text_vector_function(text = text_vector, word_vector = word_vectors)
      text_matrix_min[j, ] <- apply(text_vectorized, 2, min)
      text_matrix_max[j, ] <- apply(text_vectorized, 2, max)
      text_matrix_mean[j, ] <- apply(text_vectorized, 2, mean)
    }
    
    text_output[[i]] <- list(MIN=text_matrix_min, MAX=text_matrix_max, MEAN=text_matrix_mean)
  }
  # feature$label<-factor(data$AuthorID)
  return(list(CLUSTER.ID = data$matrix_label, TEXTMATRIX = text_output))
}


# After we map a text to vector of 50 dimensions, we calculate the Euclidean distance
# for MIN, MAX and MEAN respectively and output a list with cluster.id and 9 distance matrix
text_feature <- function(data){
  distance <- list()
  distance1 <- list()
  for (i in 1:3){
    for (j in 1:3){
      distance1[[j]] <- as.matrix(dist(data.frame(data$TEXTMATRIX[[i]][j])))
    }
    names(distance1) <- c("MIN", "MAX", "MEAN")
    distance[i] <- list(distance1)
  }
  return(list(CLUSTER.ID = data$CLUSTER.ID, DISTANCE = distance))
}

```

##1.2 Baseline Evaluation
```{r}
source("../lib/evaluation_measures.R")
source("../lib/kai_source99.R")

# Input: file names
# Output: a vector contains "precision", "recall", "f1", "accuracy", 
# and the three final parameters
one_step_5 <- function(file_names){
  
  SOMEONE <- read.csv(file_names, as.is = T)
  SOMEONE <- SOMEONE[ifelse(rowSums(SOMEONE == "") > 0, F, T), ]
  
  SOMEONE_raw <- data.frame(Coauthor = SOMEONE$Coauthor,
                            Paper = SOMEONE$Paper, 
                            Journal = SOMEONE$Journal)
  # colnames(AKumar_raw) <- c("Coauthor","Paper","Journal")
  True_labels <- SOMEONE$AuthorID
  
  setmember <- nrow(SOMEONE)
  
  ### training
  trainingnumber <- ceiling(setmember * 0.5)
  train.id <- sample(1:setmember, trainingnumber)
  training_SOMEONE <- SOMEONE_raw[train.id,]
 
  
 # raw_data <- training_SOMEONE
  True_labels_train <- SOMEONE$AuthorID[train.id]
  
  ag5_SOMEONE <- algorithm_paper_5(raw_data = training_SOMEONE, 
                                   True_labels = True_labels_train,
                                   stepsize = 0.1, epi = 0.03)
  
  
  test_SOMEONE <- SOMEONE_raw[-train.id,]
  test_true_label <- SOMEONE$AuthorID[-train.id]
  KK <- length(unique(test_true_label))
  
  test_our_label <- test_comeon_iamlazy(test_SOMEONE, 
                                        ag5_SOMEONE$best[1+ag5_SOMEONE$iter,], KK)
  perform <- performance_statistics(matching_matrix(test_true_label, test_our_label))
  return(c(perform$precision, perform$recall, perform$f1, perform$accuracy, 
           ag5_SOMEONE$best[1+ag5_SOMEONE$iter,1], ag5_SOMEONE$best[1+ag5_SOMEONE$iter,2],
           ag5_SOMEONE$best[1+ag5_SOMEONE$iter,3]))
}


Filesname <- c("AGupta","AKumar","CChen","DJohnson","JLee","JMartin","JRobinson",
               "JSmith","KTanaka","MBrown","MJones","MMiler","SLee","Ychen")
files <- paste0("../output/Coauthor_No_Space/",Filesname,".csv")
time1 <- system.time(perform1 <- one_step_5(files[1]))
time2 <- system.time(perform2 <- one_step_5(files[2]))
time3 <- system.time(perform3 <- one_step_5(files[3]))
time4 <- system.time(perform4 <- one_step_5(files[4]))
time5 <- system.time(perform5 <- one_step_5(files[5]))
time6 <- system.time(perform6 <- one_step_5(files[6]))
time7 <- system.time(perform7 <- one_step_5(files[7]))
time8 <- system.time(perform8 <- one_step_5(files[8]))
time9 <- system.time(perform9 <- one_step_5(files[9]))
time10 <- system.time(perform10 <- one_step_5(files[10]))
time11 <- system.time(perform11 <- one_step_5(files[11]))
time12 <- system.time(perform12 <- one_step_5(files[12]))
time13 <- system.time(perform13 <- one_step_5(files[13]))
time14 <- system.time(perform14 <- one_step_5(files[14]))

performance_base <- rbind(perform1, perform2, perform3, perform4, perform5,
                          perform6, perform7, perform8, perform9,
                      perform10, perform11, perform12, perform13, perform14)
colnames(performance_base) <- c("precision", "recall", "f1", "accuracy", "para_coauthor",
                                "para_paper", "para_journal") 
rownames(performance_base) <- Filesname
```

##1.3 Improved Model

```{r}
source("../lib/evaluation_measures.R")
source("../lib/kai_source992.R")

one_step_5 <- function(file_names){
  
  SOMEONE <- read.csv(file_names, as.is = T)
  SOMEONE <- SOMEONE[ifelse(rowSums(SOMEONE == "") > 0, F, T), ]
  
  SOMEONE_raw <- data.frame(Coauthor = SOMEONE$Coauthor,
                            Paper = SOMEONE$Paper, 
                            Journal = SOMEONE$Journal)
  # colnames(AKumar_raw) <- c("Coauthor","Paper","Journal")
  True_labels <- SOMEONE$AuthorID
  
  setmember <- nrow(SOMEONE)
  
  ### training
  trainingnumber <- ceiling(setmember * 0.5)
  train.id <- sample(1:setmember, trainingnumber)
  training_SOMEONE <- SOMEONE_raw[train.id,]
 
 # dim(training_SOMEONE)
#  dim(test_SOMEONE)
  
 # raw_data <- training_SOMEONE
  True_labels_train <- SOMEONE$AuthorID[train.id]
  
  ag5_SOMEONE <- algorithm_paper_5(raw_data = training_SOMEONE, 
                                   True_labels = True_labels_train,
                                   stepsize = 0.1, epi = 0.03)
  
  # sb <- NULL
  
  
  test_SOMEONE <- SOMEONE_raw[-train.id,]
  test_true_label <- SOMEONE$AuthorID[-train.id]
  KK <- length(unique(test_true_label))
  
  test_our_label <- test_comeon_iamlazy(test_SOMEONE, 
                                        ag5_SOMEONE$best[1+ag5_SOMEONE$iter,], KK)
  perform <- performance_statistics(matching_matrix(test_true_label, test_our_label))
  return(c(perform$precision, perform$recall, perform$f1, perform$accuracy, 
           ag5_SOMEONE$best[1+ag5_SOMEONE$iter,1], ag5_SOMEONE$best[1+ag5_SOMEONE$iter,2],
           ag5_SOMEONE$best[1+ag5_SOMEONE$iter,3], ag5_SOMEONE$best[1+ag5_SOMEONE$iter,4],
           ag5_SOMEONE$best[1+ag5_SOMEONE$iter,5], ag5_SOMEONE$best[1+ag5_SOMEONE$iter,6]))
}

Filesname <- c("AGupta","AKumar","CChen","DJohnson","JLee","JMartin","JRobinson",
               "JSmith","KTanaka","MBrown","MJones","MMiler","SLee","Ychen")
files <- paste0("../output/Coauthor_No_Space/",Filesname,".csv")
time_1 <- system.time(perform1 <- one_step_5(files[1]))
time_2 <- system.time(perform2 <- one_step_5(files[2]))
time_3 <- system.time(perform3 <- one_step_5(files[3]))
time_4 <- system.time(perform4 <- one_step_5(files[4]))
time_5 <- system.time(perform5 <- one_step_5(files[5]))
time_6 <- system.time(perform6 <- one_step_5(files[6]))
time_7 <- system.time(perform7 <- one_step_5(files[7]))
time_8 <- system.time(perform8 <- one_step_5(files[8]))
time_9 <- system.time(perform9 <- one_step_5(files[9]))
time_10 <- system.time(perform10 <- one_step_5(files[10]))
time_11 <- system.time(perform11 <- one_step_5(files[11]))
time_12 <- system.time(perform12 <- one_step_5(files[12]))
time_13 <- system.time(perform13 <- one_step_5(files[13]))
time_14 <- system.time(perform14 <- one_step_5(files[14]))

improved_df <- rbind(perform1,perform2, perform3, perform4, perform5,perform6, 
                     perform7, perform8, perform9, perform10,
                     perform11, perform12, perform13, perform14)
colnames(improved_df) <- c("precision2", "recall2", "f1_2", "accuracy2", 
                           "para_cos_coauthor2", "para_cos_paper2", "para_cos_journal2",
                           "para_dis_coauthor2", "para_dist_paper2", "para_dist_journal2")

rownames(improved_df) <- Filesname
```


##1.3 Model Comparison
```{r}
new_result <- data.frame(baseline, improved_df)
new_result$Author <- rep(1:14)

plot(x=new_result$Author, y=new_result$precision, type = "o", 
     ylim = c(0,1), xlab = "Author", ylab = "Precision", main = "Precision Comparison")
lines(x=new_result$Author, y=new_result$precision2, col="red")
legend("topright", legend = c("Baseline", "Improved"), fill = c("black", "red"), cex = 0.5)

plot(x=new_result$Author, y=new_result$recall, type = "o", 
     ylim = c(0,1), xlab = "Author", ylab = "Recall", main = "Recall Comparison")
lines(x=new_result$Author, y=new_result$recall2, col="red")
legend("bottomright", legend = c("Baseline", "Improved"), fill = c("black", "red"), cex = 0.5)

plot(x=new_result$Author, y=new_result$f1, type = "o", 
     ylim = c(0,1), xlab = "Author", ylab = "f1", main = "F1 Comparison")
lines(x=new_result$Author, y=new_result$f1_2, col="red")
legend("topright", legend = c("Baseline", "Improved"), fill = c("black", "red"), cex = 0.5)

plot(x=new_result$Author, y=new_result$accuracy, type = "o", 
     ylim = c(0,1), xlab = "Author", ylab = "Accuracy", main = "Accuracy Comparison")
lines(x=new_result$Author, y=new_result$accuracy2, col="red")
legend("bottomright", legend = c("Baseline", "Improved"), fill = c("black", "red"), cex = 0.5)

```


###Observe the parameters for baseline

```{r}
new_result[,c("para_coauthor", "para_paper", "para_journal")]
```

###Observe the parameters for improved model
```{r}
new_result[,c("para_cos_coauthor2", "para_cos_paper2", 
              "para_cos_journal2", "para_dis_coauthor2", 
              "para_dist_paper2", "para_dist_journal2")]
```
